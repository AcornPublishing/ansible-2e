[[ch_docker]]
== Docker

[role="footnotereset1"]
The Docker project has taken the IT world by storm. I can't think of another
technology that was so quickly embraced by the community.((("Docker", id="ix_Dock")))  This chapter covers
how to use Ansible to create Docker images and deploy Docker
containers.

.What Is a Container?
****
A _container_ is a form of virtualization.((("virtualization")))((("containers"))) When you use virtualization to run processes in a guest operating system, these guest processes have no visibility into the host operating system that runs on the physical hardware. In particular, processes running in the guest are not able to directly access physical resources, even if these guest processes are provided with the illusion that they have root access.

Containers are sometimes referred to as _operating system virtualization_ to distinguish them from _hardware virtualization_ technologies.((("operating system virtualization")))((("hardware virtualization"))) In hardware virtualization, a program called the _hypervisor_ virtualizes an
entire physical machine, including a virtualized CPU, memory, and devices such
as disks and network interfaces.((("hypervisors"))) Because the entire machine is virtualized,
hardware virtualization is flexible. In particular, you can run an entirely
different operating system in the guest than in the host (e.g., running a
Windows Server 2012 guest inside a Red Hat Enterprise Linux host), and you can
suspend and resume a virtual machine just as you can a physical machine. This
flexibility brings with it additional overhead needed to virtualize the
hardware.

With operating system virtualization (containers), the guest processes
are isolated from the host by the operating system. The guest processes run on the same kernel as the host. The host operating system is responsible for ensuring that the guest processes are fully isolated from the host. When running a Linux-based container program such as Docker, the guest processes also must be Linux programs. However, the overhead is much lower than that of hardware virtualization, because you are running only a single operating system. In particular, processes start up much more quickly inside containers than inside virtual machines.
****

Docker is more than just containers.((("containers", "Docker versus")))((("Docker", "containers versus"))) Think of Docker as being a platform where
containers are a building block. To use an analogy, containers are to Docker
what virtual machines are to IaaS clouds. The other two major pieces that make up Docker are
its image format and the Docker API.

You can think of Docker images as similar to virtual machine images.((("images (Docker)")))((("Docker", "images"))) A Docker
image contains a filesystem with an installed operating system, along with some
metadata. One important difference is that Docker images are
layered. You create a new Docker image by taking an
existing Docker image and customizing it by adding, modifying, and
deleting files. The representation for the new Docker image contains a
reference to the original Docker image, as well as the filesystem differences
between the original Docker image and the new Docker image.((("Nginx", "Docker image")))  As an example, the
official http://bit.ly/2ktXbqS[Nginx docker image] is built as layers on top of
the official Debian Jessie image. The layered approach means that Docker images are smaller than traditional virtual machine images, so it's faster to transfer Docker images over the internet than it would be to transfer a traditional virtual machine image. The Docker project maintains a registry of publicly https://registry.hub.docker.com[available images].

Docker also supports a remote API, which enables third-party tools to interact with it.((("Docker", "remote API"))) In particular, Ansible's +docker+ module uses the Docker remote API.

=== The Case for Pairing Docker with Ansible

Docker containers make it easier to package your application into a single image that's easy to deploy in different places,((("Docker", "case for pairing with Ansible")))((("Ansible", "pairing with Docker"))) which is why the Docker project has embraced the metaphor of the shipping container. Docker's remote API  simplifies the automation of software systems that run on top of Docker.

Ansible simplifies working with Docker in two areas. One is in the
orchestration of Docker containers.((("containers", "Docker, orchestration with Ansible"))) When you deploy a "Dockerized" software app,
you're typically creating multiple Docker containers that contain different
services. These services need to communicate with each other, so you need to
connect the appropriate containers correctly and ensure they start up in the
right order. Initially, the Docker project did not provide orchestration
tools, so third-party tools emerged to fill in the gap. Ansible was built for
doing orchestration, so it's a natural fit for deploying your Docker-based
application.


The other area is the creation of Docker images. The official way to create your
own Docker images is by writing special text files called _Dockerfiles_, which
resemble shell scripts.((("Dockerfiles")))((("images (Docker)", "creating"))) For simpler images, Dockerfiles work just fine.
However, when you start to create more-complex images, you'll quickly miss the
power that Ansible provides. Fortunately, you can use Ansible to create
playbooks.


[NOTE]
====
A new project called _Ansible Container_ is the official approach
for using Ansible playbooks  to build Docker container images.((("Ansible Container"))) At the time this book was
written, the latest release of Ansible Container is 0.2. On January 29, 2017, the project maintainers
announced on the Ansible Container mailing list that the next release of
the project, dubbed _Ansible Container Mk. II_, will be substantially pass:[<span class="keep-together">different</span>].

Because Ansible Container is still in flux, we chose not to cover it here.
However, we do recommend that you take a look at this project once it has stabilized.
====

=== Docker Application Life Cycle

Here's what the typical life cycle of a Docker-based((("Docker", "application life cycle"))) application looks like:

 . Create Docker images on your local machine.
 . Push Docker images up from your local machine to the registry.
 . Pull Docker images down to your remote hosts from the registry.
 . Start up Docker containers on the remote hosts, passing in any
   configuration information to the containers on startup.

You typically create your Docker image on your local
machine, or on a continuous integration system that supports creating Docker
images, such as Jenkins or CircleCI. Once you've created your image, you need to
store it somewhere that will be convenient for downloading onto your remote
hosts.

Docker images typically reside in a repository called a _registry_.((("registry (Docker images)")))((("images (Docker)", "in registry"))) The
Docker project runs a registry called _Docker Hub_, which can host both public
and private Docker images, and where the Docker command-line tools have built-in
support for pushing images up to a registry and for pulling images down from a
registry.((("Docker Hub")))

Once your Docker image is in the registry, you connect to a remote host, pull
down the container image, and then run the container. Note that if you try to
run a container whose image isn't on the host, Docker will automatically
pull down the image from the registry,
so you do not need to explicitly issue a command to download an image from the
registry.


When you use Ansible to create the Docker images and start the containers
on the remote hosts, the application life cycle looks like this:

[role="pagebreak-before"]
 . Write Ansible playbooks for creating Docker images.
 . Run the playbooks to create Docker images on your local machine.
 . Push Docker images up from your local machine to the registry.
 . Write Ansible playbooks to pull Docker images down to remote hosts and start up Docker containers on remote hosts, passing in configuration information.
 . Run Ansible playbooks to start up the containers.

=== Example Application: Ghost

In this chapter, we're going to switch from Mezzanine to Ghost as our example
application. ((("Docker", "example application, Ghost")))((("Ghost")))Ghost is an open source blogging platform, similar to WordPress.
The Ghost project has an official Docker container that we'll be using.

What we'll cover in this chapter:

* Running a Ghost container on your local machine
* Running a Ghost container fronted by an Nginx container with SSL configured
* Pushing a custom Nginx image to a registry
* Deploying our Ghost and Nginx containers to a remote machine

=== Connecting to the Docker Daemon

All of the Ansible Docker modules communicate with the Docker daemon.((("Docker", "connecting to Docker daemon"))) If you are
running on Linux, or if you are running on macOS using Docker for Mac, all of
the modules should just work without passing additional arguments.

If you are running on macOS using Boot2Docker or Docker Machine, or for
other cases where the machine that executes the module is not the same as the
machine that is running the Docker daemon, you may need to pass extra information
to the Docker modules so they can reach the Docker daemon.((("macOS", "Docker connection options")))
<<docker_connection_options>> lists these options, which can be passed as either
module arguments or environment variables.((("docker_container module"))) See the +docker_container+ module
documentation for more details about what these options do.

[[docker_connection_options]]
.Docker connection options
[options="header"]
|====================================================================
|Module argument |Environment variable | Default
|docker_host     |DOCKER_HOST          | +unix://var/run/docker.sock+
|tls_hostname    |DOCKER_TLS_HOSTNAME  | +localhost+
|api_version     |DOCKER_API_VERSION   | +auto+
|cert_path       |DOCKER_CERT_PATH     | (_None_)
|ssl_version     |DOCKER_SSL_VERSION   | (_None_)
|tls             |DOCKER_TLS           | +no+
|tls_verify      |DOCKER_TLS_VERIFY    | +no+
|timeout         |DOCKER_TIMEOUT       | +60+ (seconds)
|====================================================================

=== Running a Container on Our Local Machine

The +docker_container+ module starts and stops Docker containers, implementing
some of the functionality of the +docker+ command-line tool such as the `run`, `kill`, and
`rm` commands.((("docker command-line tool")))((("docker_container module", "starting and stopping containers")))((("containers", "Docker, running on local machine")))

Assuming you have Docker installed locally, the following invocation will download the ghost
image from the Docker registry and execute it locally.((("Ghost", "downloading image from Docker container and running locally"))) It will map port 2368
inside the container to 8000 on your machine, so you can access Ghost at
pass:[<em>http://localhost:8000</em>].

----
$ ansible localhost -m docker_container -a "name=test-ghost image=ghost \
  ports=8000:2368"
----

The first time you run this, it may take some time for Docker to download the
image.((("docker ps command"))) If it succeeds, the +docker ps+ command will show the running container:

----
$ docker ps
CONTAINER ID   IMAGE           COMMAND                  CREATED
48e69da90023   ghost           "/entrypoint.sh np..."   37 seconds ago
               STATUS          PORTS                    NAMES
               Up 36 seconds   0.0.0.0:8000->2368/tcp   test-ghost
----

To stop and remove the container:

----
$ ansible localhost -m docker_container -a "name=test-ghost state=absent"
----

The +docker_container+ module supports many options: if you can
pass an argument by using the +docker+ command-line tool, you're likely to find an
equivalent option on the module.((("docker_container module", "support for many options")))

=== Building an Image from a Dockerfile

The stock Ghost image works great on its own, but if we want to ensure that access is
secure, we'll need to front it with a web server configured for TLS.((("Docker", "building an image from a Dockerfile", id="ix_Dockimg")))((("images (Docker)", "building from a Dockerfile", id="ix_imgDock")))

The Nginx project puts out a stock Nginx image,((("Nginx", "configuration to be frontend for Ghost")))((("Ghost", "Nginx configuration as frontend for"))) but we'll need to configure it
to act as a frontend for Ghost and to enable TLS, similar to the way we did it in
<<deploying_mezzanine>> for Mezzanine. <<Dockerfile>> shows the Dockerfile for
this.

[[Dockerfile]]
.Dockerfile
====
----
FROM nginx
RUN rm /etc/nginx/conf.d/default.conf
COPY ghost.conf /etc/nginx/conf.d/ghost.conf
----
====

<<ghost.conf>> shows the Nginx configuration for being a frontend for Ghost.
The main difference between this one and the one for Mezzanine is that in this
case Nginx is communicating with Ghost by using a TCP socket (port 2368), whereas
in the Mezzanine case the communication was over a Unix domain socket.

The other difference is((("TLS (Transport Layer Security)", "Nginx configuration for"))) that the path containing the TLS files is _/certs_.

[[ghost.conf]]
.ghost.conf
====
----
upstream ghost {
    server ghost:2368;
}

server {

    listen 80;

    listen 443 ssl;

    client_max_body_size 10M;
    keepalive_timeout    15;

    ssl_certificate      /certs/nginx.crt;
    ssl_certificate_key  /certs/nginx.key;
    ssl_session_cache    shared:SSL:10m;
    ssl_session_timeout  10m;
    # # ssl_ciphers entry is too long to show in this book
    ssl_prefer_server_ciphers on;

    location / {
        proxy_redirect      off;
        proxy_set_header    Host                    $host;
        proxy_set_header    X-Real-IP               $remote_addr;
        proxy_set_header    X-Forwarded-For         $proxy_add_x_forwarded_for;
        proxy_set_header    X-Forwarded-Protocol    $scheme;
        proxy_pass          http://ghost;
    }
}
----
====

This configuration assumes that Nginx can reach the Ghost server via the
hostname `ghost`. When we deploy these containers, we must ensure that this is
the case; otherwise, the Nginx container will not be able to reach the Ghost
container.

Assuming we put the Dockerfile and _nginx.conf_ file in a directory named _nginx_,
this task will create an image named _lorin/nginx-ghost_. We use the prefix
_ansiblebook/_ since this will eventually be pushed to the
_ansiblebook/nginx-ghost_ Docker Hub pass:[<span class="keep-together">repository</span>]:

----
- name: create Nginx image
  docker_image:
    name: ansiblebook/nginx-ghost
    path: nginx
----

We can confirm ((("docker images command")))this with the +docker images+ command:

----
$ docker images
REPOSITORY                TAG                 IMAGE ID            CREATED
ansiblebook/nginx-ghost   latest              23fd848947a7        37 seconds ago
ghost                     latest              066a22d980f4        3 days ago
nginx                     latest              cc1b61406712        11 days ago
                          SIZE
                          182 MB
                          326 MB
                          182 MB
----


Note that invoking the +docker_image+ module to build an image will have no
effect if an image with that name already exists, even if you've made changes to
the Dockerfile.((("docker_images module"))) If you've made changes to the Dockerfile and want to rebuild, you need to add the
+force: yes+ option.

In general, though, it's a good idea to add a +tag+ option with a version
number, and increment this each time you do a new build. The +docker_image+
module would then build the new image without needing to be forced.((("Docker", "building an image from a Dockerfile", startref="ix_Dockimg")))((("images (Docker)", "building from a Dockerfile", startref="ix_imgDock")))

=== Orchestrating Multiple Containers on Our Local Machine

It's common to run multiple Docker containers and wire them up together.((("Docker", "orchestrating multiple containers on local machine")))((("containers", "orchestrating multiple Docker containers on local machine"))) During
development, you typically run all of these containers together on your local
machine. In production, these containers are commonly hosted on
different machines.

For local development where all of the containers are running on the same
machine, the Docker project has a tool called _Docker Compose_ that makes it
simpler to bring the containers up and wire them together.((("Docker Compose tool")))((("docker_service module"))) The +docker_service+
module can be used to control Docker Compose to bring the services up or down.

<<docker-compose.yml>> shows a _docker-compose.yml_ file that will start up Nginx
and Ghost. ((("Nginx", "docker-compose file starting up")))((("Ghost", "docker-compose file starting up")))The file assumes there's a _./certs_ directory that contains the TLS
certificate files.((("TLS (Transport Layer Security)", "certificates")))((("certificates", "directory containing TLS certificates")))


[[docker-compose.yml]]
.docker-compose.yml
====
----
version: '2'
services:
  nginx:
    image: ansiblebook/nginx-ghost
    ports:
      - "8000:80"
      - "8443:443"
    volumes:
      - ${PWD}/certs:/certs
    links:
      - ghost
  ghost:
    image: ghost
----
====

<<local.yml>> shows a playback that creates the custom Nginx image file, creates
self-signed certificates, and then starts up the services specified by
<<docker-compose.yml>>.

[[local.yml]]
.ghost.yml
====
----
---
- name: Run Ghost locally
  hosts: localhost
  gather_facts: False
  tasks:
    - name: create Nginx image
      docker_image:
        name: ansiblebook/nginx-ghost
        path: nginx
    - name: create certs
      command: >
        openssl req -new -x509 -nodes
        -out certs/nginx.crt -keyout certs/nginx.key
        -subj '/CN=localhost' -days 3650
        creates=certs/nginx.crt
    - name: bring up services
      docker_service:
        project_src: .
        state: present
----
====


=== Pushing Our Image to the Docker Registry

We'll use a separate playbook to publish our image to Docker Hub; it's shown as
<<publish.yml>>.((("images (Docker)", "pushing our image to Docker registry", id="ix_imgDockpush")))((("Docker", "pushing our image to Docker registry", id="ix_Dockimgreg")))((("registry (Docker images)", "pushing our image to", id="ix_registry"))) Note that the +docker_login+ module must be invoked first to
log in to the registry before the image is to be pushed.((("docker_login module"))) The +docker_login+ and
+docker_image+ modules both default to Docker Hub((("Docker Hub"))) as the registry.



[[publish.yml]]
.publish.yml
====
----
- name: publish images to docker hub
  hosts: localhost
  gather_facts: False
  vars_prompt:
    - name: username
      prompt: Enter Docker Registry username
    - name: email
      prompt: Enter Docker Registry email
    - name: password
      prompt: Enter Docker Registry password
      private: yes
  tasks:
    - name: authenticate with repository
      docker_login:
        username: "{{ username }}"
        email: "{{ email }}"
        password: "{{ password }}"
    - name: push image up
      docker_image:
        name: ansiblebook/nginx-ghost
        push: yes
----
====

If you wish to use a different registry, specify a +registry_url+ option to +docker_login+
and prefix the image name with the hostname and port (if not using the standard
HTTP/HTTPS port) of the registry.  <<publish-yml-alternate-registry>> shows how the tasks would change when using a
registry at pass:[<em>http://reg.example.com</em>]. The playbook for creating the image would
also need to change to reflect the new name of the image:
_reg.example.com/ansiblebook/nginx-ghost_.

[[publish-yml-alternate-registry]]
.publish.yml with custom registry
====
----
  tasks:
    - name: authenticate with repository
      docker_login:
        username: "{{ username }}"
        email: "{{ email }}"
        password: "{{ password }}"
        registry_url: http://reg.example.com
    - name: push image up
      docker_image:
        name: reg.example.com/ansiblebook/nginx-ghost
        push: yes
----
====

We can test pushing to Docker registries by using a local registry.
<<publish-local.yml>> starts a registry inside a Docker container, tags
the _ansiblebook/nginx-ghost_ image as _localhost:5000/ansiblebook/nginx-ghost_,
and pushes it to the registry. Note that the local registry doesn't require
authentication by default, so there's no task that involves +docker_login+ in
this playbook.

[[publish-local.yml]]
.publish.yml with a local registry
====
----
- name: publish images to local docker registry
  hosts: localhost
  gather_facts: False
  vars:
    repo_port: 5000
    repo: "localhost:{{repo_port}}"
    image: ansiblebook/nginx-ghost
  tasks:
    - name: start a registry locally
      docker_container:
        name: registry
        image: registry:2
        ports: "{{ repo_port }}:5000"
    - debug:
        msg: name={{ image }} repo={{ repo }}/{{ image }}
    - name: tag the nginx-ghost image to the repository
      docker_image:
        name: "{{ image }}"
        repository: "{{ repo }}/{{ image }}"
        push: yes
----
====

We can verify the upload worked by downloading the manifest:

----
$ curl http://localhost:5000/v2/ansiblebook/nginx-ghost/manifests/latest
{
   "schemaVersion": 1,
   "name": "ansiblebook/nginx-ghost",
   "tag": "latest",
   ...
}
----

=== Querying Local Images

The +docker_image_facts+ module allows you to query the metadata on a locally
stored image.((("images (Docker)", "pushing our image to Docker registry", startref="ix_imgDockpush")))((("Docker", "pushing our image to Docker registry", startref="ix_Dockimgreg")))((("registry (Docker images)", "pushing our image to"))) <<image-facts.yml>> shows an example of a playbook that uses this
module to query the ghost image for the exposed port and volumes.((("Docker", "querying local images", id="ix_Dockquery")))((("images (Docker)", "querying local images", id="ix_imgDockquery")))

[[image-facts.yml]]
.image-facts.yml
====
----
---
- name: get exposed ports and volumes
  hosts: localhost
  gather_facts: False
  vars:
    image: ghost
  tasks:
    - name: get image info
      docker_image_facts: name=ghost
      register: ghost
    - name: extract ports
      set_fact:
        ports: "{{ ghost.images[0].Config.ExposedPorts.keys() }}"
    - name: we expect only one port to be exposed
      assert:
        that: "ports|length == 1"
    - name: output exposed port
      debug:
        msg: "Exposed port: {{ ports[0] }}"
    - name: extract volumes
      set_fact:
        volumes: "{{ ghost.images[0].Config.Volumes.keys() }}"
    - name: output volumes
      debug:
        msg: "Volume: {{ item }}"
      with_items: "{{ volumes }}"
----
====

The output looks like this:

----
$ ansible-playbook image-facts.yml

PLAY [get exposed ports and volumes] *******************************************

TASK [get image info] **********************************************************
ok: [localhost]

TASK [extract ports] ***********************************************************
ok: [localhost]

TASK [we expect only one port to be exposed] ***********************************
ok: [localhost] => {
    "changed": false,
    "msg": "All assertions passed"
}

TASK [output exposed port] *****************************************************
ok: [localhost] => {
    "msg": "Exposed port: 2368/tcp"
}

TASK [extract volumes] *********************************************************
ok: [localhost]

TASK [output volumes] **********************************************************
ok: [localhost] => (item=/var/lib/ghost) => {
    "item": "/var/lib/ghost",
    "msg": "Volume: /var/lib/ghost"
}

PLAY RECAP *********************************************************************
localhost                  : ok=6    changed=0    unreachable=0    failed=0
----


=== Deploying the Dockerized Application

By default, Ghost uses SQLite as its database backend.((("Docker", "querying local images", startref="x_Dockquery")))((("images (Docker)", "querying local images", startref="ix_imgDockquery"))) For deployment, we're
going to use Postgres as the database backend, for the reasons we
discussed in <<introducing_mezzanine>>.((("Ghost", "deploying Dockerized application", id="ix_Ghostdep")))((("Docker", "deploying Dockerized application", id="ix_Dockdep")))

We're going to deploy onto two separate machines. One machine (+ghost+)
will run the Ghost container and the Nginx container. The other machine
(+postgres+) will run a Postgres container that will serve as a persistent store
for the Ghost data.

This example assumes the following variables are defined somewhere such as
_group_vars/all_, where they are in scope for the frontend and backend
machines:

* +database_name+
* +database_user+
* +database_password+


==== Backend: Postgres

To configure the Postgres container, we need to pass the database user, database
password, and database name as environment variables that the container expects.
We also want to mount a directory from a host machine as a volume for storing
the persistent data, because we don't want our persistent data to disappear if
the container stops and gets removed.

<<deploy_postgres>> shows the playbook for deploying the Postgres container. It
has only two tasks: one to create the directory that will hold the data, and the
other to start the Postgres container. Note that this playbook assumes that
Docker Engine is already installed on the `postgres` host.


[[deploy_postgres]]
.postgres.yml
====
----
- name: deploy postgres
  hosts: postgres
  become: True
  gather_facts: False
  vars:
    data_dir: /data/pgdata
  tasks:
    - name: create data dir with correct ownership
      file:
        path: "{{ data_dir }}"
        state: directory
    - name: start postgres container
      docker_container:
        name: postgres_ghost
        image: postgres:9.6
        ports:
          - "0.0.0.0:5432:5432"
        volumes:
          - "{{ data_dir }}:/var/lib/postgresql/data"
        env:
          POSTGRES_USER: "{{ database_user }}"
          POSTGRES_PASSWORD: "{{ database_password }}"
          POSTGRES_DB: "{{ database_name }}"
----
====

==== Frontend

The frontend deployment is more complex, since we have two containers to
deploy: Ghost and Nginx.((("Docker", "deploying Dockerized application", "frontend"))) We also need to wire them up, and we need to pass
configuration information to the Ghost container so it can access the Postgres
database.

We're going to use Docker networks to enable the Nginx container to connect to
the Ghost container.((("containers", "enabling Nginx container to connect to Ghost container")))((("Nginx", "enabling Nginx container to connect to Ghost container")))((("networks (Docker), creating"))) Networks replace the legacy `links` functionality that was
previously used for connecting containers. Using Docker networks, you
create a custom Docker network, attach containers to that network, and the
containers can access each other by using the container names as hostnames.

Creating a Docker network is simple:

----
- name: create network
  docker_network: name=ghostnet
----

It makes more sense to use a variable for the network name, since we'll need to
reference it for each container we bring up. This is how our playbook will
start:

----
- name: deploy ghost
  hosts: ghost
  become: True
  gather_facts: False
  vars:
    url: "https://{{ ansible_host }}"
    database_host: "{{ groups['postgres'][0] }}"
    data_dir: /data/ghostdata
    certs_dir: /data/certs
    net_name: ghostnet
  tasks:
    - name: create network
      docker_network: "name={{ net_name }}"
----

Note that this playbook assumes there's a group named `postgres` that contains a
single host; it uses this information to populate the +database_host+ variable.


==== Frontend: Ghost

We need to configure Ghost to connect to the Postgres database, as well as to
run in production mode by passing the +--production+ flag to the +npm start+
command.((("Ghost", "deploying Dockerized application", "frontend, Ghost")))((("Docker", "deploying Dockerized application", "frontend, Ghost")))

We also want to ensure that the persistent files that it generates are written to a
volume mount.

Here's the part of the playbook that creates the directory that will hold the
persistent data, generates a Ghost config file from a template, and starts up
the container, connected to the +ghostnet+ network:

----
    - name: create ghostdata directory
      file:
        path: "{{ data_dir }}"
        state: directory
    - name: generate the config file
      template: src=templates/config.js.j2 dest={{ data_dir }}/config.js
    - name: start ghost container
      docker_container:
        name: ghost
        image: ghost
        command: npm start --production
        volumes:
          - "{{ data_dir }}:/var/lib/ghost"
        networks:
          - name: "{{ net_name }}"
----

Note that we don't need to publish any ports here, since only the Nginx
container will communicate with the Ghost container.


==== Frontend: Nginx

The Nginx container had its configuration hardwired into it when we created the
_ansiblebook/nginx-ghost_ image: it is configured to connect to +ghost:2368+.((("Ghost", "deploying Dockerized application", "frontend, Nginx")))((("Docker", "deploying Dockerized application", "frontend, Nginx")))((("Nginx", "frontend for Dockerized Ghost application")))

However, we do need to copy the TLS certificates. ((("certificates", "generating self-signed TLS certificates")))((("TLS (Transport Layer Security)", "certificates", "generating self-signed certificates")))As in previous examples, we'll
just generate self-signed certificates:

----
    - name: create certs directory
      file:
        path: "{{ certs_dir }}"
        state: directory
    - name: generate tls certs
      command: >
        openssl req -new -x509 -nodes
        -out "{{ certs_dir }}/nginx.crt" -keyout "{{ certs_dir }}/nginx.key"
        -subj "/CN={{ ansible_host}}" -days 3650
        creates=certs/nginx.crt
    - name: start nginx container
      docker_container:
        name: nginx_ghost
        image: ansiblebook/nginx-ghost
        pull: yes
        networks:
          - name: "{{ net_name }}"
        ports:
          - "0.0.0.0:80:80"
          - "0.0.0.0:443:443"
        volumes:
          - "{{ certs_dir }}:/certs"
----

==== Cleaning Out Containers

Ansible makes it easy to stop and remove containers, which is useful
when you're developing and testing deployment scripts.((("containers", "cleaning out Ghost containers")))((("Docker", "deploying Dockerized application", "cleaning out containers")))((("Ghost", "deploying Dockerized application", "cleaning out containers"))) Here is a playbook
that cleans up the +ghost+ host.

----
- name: remove all ghost containers and networks
  hosts: ghost
  become: True
  gather_facts: False
  tasks:
    - name: remove containers
      docker_container:
        name: "{{ item }}"
        state: absent
      with_items:
        - nginx_ghost
        - ghost
    - name: remove network
      docker_network:
        name: ghostnet
        state: absent
----

==== Connecting Directly to Containers

Ansible has support for interacting directly with running containers.((("Ghost", "deploying Dockerized application", "connecting directly to containers")))((("Docker", "deploying Dockerized application", "connecting directly to containers")))((("containers", "connecting directly to, using Docker inventory plugin"))) Ansible's
Docker inventory plugin will automatically generate an inventory of accessible
running hosts, and its Docker connection plugin does the equivalent of +docker
exec+ to execute processes in the context of a running container.((("inventory", "Docker inventory plugin")))

The Docker inventory plugin is available in the GitHub _ansible/ansible_ repo at
_contrib/inventory/docker.py_.((("GitHub repositories", "Docker inventory plugin"))) By default, this plugin accesses the Docker daemon
running on your local machine. It can be configured to connect to Docker daemons
on remote machines using Docker's REST API, or to connect to running Docker
containers that have an SSH server running inside them. Both of these require additional setup work. To access the Docker API remotely, the host running
Docker must be configured to bind to a TCP port. To connect to a container via
SSH, the container must be configured to start up an SSH server. We don't cover
those scenarios here, but you can check out the example configuration file in
the repo at _contrib/inventory/docker.yml_.

Assuming we have the following containers running locally:

----
CONTAINER ID        IMAGE                    NAMES
63b6767de77f        ansiblebook/nginx-ghost  ch14_nginx_1
057d72a95016        ghost                    ch14_ghost_1
----

the _docker.py_ inventory script creates a host per name. In this case:

 - +ch14_nginx_1+
 - +ch14_ghost_1+

It also creates groups for short ID, long ID, Docker image, and a group for all
running containers. Continuing on with our example, the created groups are as follows:

 - +63b6767de77fe+ (+ch14_nginx_1+)
 - +63b6767de77fe01aa6d840dd897329766bbd3dc60409001cc36e900f8d501d6d+ (+ch14_nginx_1+)
 - +057d72a950163+ (+ch14_ghost_1+)
 - +057d72a950163769c2bcc1ecc81ba377d03c39b1d19f8f4a9f0c748230b42c5c+ (+ch14_ghost_1+)
 - +image_ansiblebook/nginx-ghost+ (+ch14_nginx_1+)
 - +image_ghost+ (+ch14_ghost_1+)
 - +running+ (+ch14_nginx_1+, +ch14_ghost_1+)

Here's how we combine the Docker dynamic inventory script with the Docker
connection plugin (enabled by passing +-c docker+ as an argument) to list all of
the processes running inside each container:

----
$ ansible -c docker running -m raw -a 'ps aux'

ch14_ghost_1 | SUCCESS | rc=0 >>
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
user         1  0.0  2.2 1077892 45040 ?       Ssl  05:19   0:00 npm
user        34  0.0  0.0   4340   804 ?        S    05:19   0:00 sh -c node ind
user        35  0.0  5.9 1255292 121728 ?      Sl   05:19   0:02 node index
root       108  0.0  0.0   4336   724 ?        Ss   06:20   0:00 /bin/sh -c ps
root       114  0.0  0.1  17500  2076 ?        R    06:20   0:00 ps aux


ch14_nginx_1 | SUCCESS | rc=0 >>
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  46320  5668 ?        Ss   05:19   0:00 nginx: master
nginx        6  0.0  0.1  46736  3020 ?        S    05:19   0:00 nginx: worker
root        71  0.0  0.0   4336   752 ?        Ss   06:20   0:00 /bin/sh -c ps
root        77  0.0  0.0  17500  2028 ?        R    06:20   0:00 ps aux
----

include::ansible-container.asciidoc[]

Docker as a technology has clearly demonstrated that it has staying power. In
this chapter, we covered how to manage Docker images, containers, and networks.
While we weren't able to cover the creation of Docker images with Ansible
playbooks, by the time you read this, you'll likely be able to use Ansible
playbooks for creating images as well.((("Ghost", "deploying Dockerized application", startref="ix_Ghostdep")))((("Docker", "deploying Dockerized application", startref="ix_Dockdep")))((("Docker", startref="ix_Dock")))

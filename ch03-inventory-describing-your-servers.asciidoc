[[inventory]]
== Inventory: Describing Your Servers

So far, we've been working with only one server (or _host_, as Ansible calls
it). In reality, you're going to be managing multiple hosts.((("hosts", "inventory", seealso="inventory")))((("inventory", id="ix_inven"))) The collection of
hosts that Ansible knows about is called the _inventory_. In this chapter, you will learn how to describe a set of hosts as an Ansible inventory. 

=== The Inventory File

The default way to describe your hosts in Ansible is to list them in text files, called _inventory files_.((("inventory", "inventory files")))((("inventory files"))) A very simple inventory file might contain only a list of hostnames, as shown in <<example3-1>>.

[[example3-1]]
.A very simple inventory file
====
----
ontario.example.com
newhampshire.example.com
maryland.example.com
virginia.example.com
newyork.example.com
quebec.example.com
rhodeisland.example.com
----
====

[NOTE]
====
Ansible uses your local SSH client by default, which means that it will
understand any aliases that you set up in your SSH config file.((("SSH", "default local client, use by Ansible"))) This does not
hold true if you configure Ansible to use the Paramiko connection plugin instead
of the default SSH plugin.
====

////
TODO: Look up what version of ansible localhost was introduced
////

Ansible automatically adds one host to the inventory by default: _localhost_. Ansible understands that +localhost+ refers to your local machine, so it will interact with it directly rather than connecting by SSH.((("localhost")))

[WARNING]
====
Although Ansible adds +localhost+ to your inventory automatically, you have to have at least one other host in your inventory file; otherwise, +ansible-playbook+ will terminate with an error:

----
ERROR: provided hosts list is empty
----

If you have no other hosts in your inventory file, you can
explicitly add an entry for +localhost+ like this:

----
localhost ansible_connection=local
----
====

[[preliminaries_multiple_vagrant]]
=== Preliminaries: Multiple Vagrant Machines

To talk about inventory, we need to interact with multiple hosts. Let's
configure Vagrant to bring up three hosts.((("inventory", "multiple Vagrant machines", id="ix_invenVag")))((("Vagrant", "configuring to bring up three hosts", id="ix_Vagmulti"))) We'll unimaginatively call them +vagrant1+, +vagrant2+, and +vagrant3+.

Before you modify your existing Vagrantfile, make sure you destroy your
existing virtual machine by running the following:

----
$ vagrant destroy --force
----

If you don't include the +--force+ option, Vagrant will prompt you to confirm that
you want to destroy the virtual machine.((("vagrant destroy --force command")))

Next, edit your Vagrantfile so it looks like <<VAGRANTFILE_WITH_THREE_SERVERS>>.

[[VAGRANTFILE_WITH_THREE_SERVERS]]
.Vagrantfile with three servers
====
[source,ruby]
----
VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  # Use the same key for each machine
  config.ssh.insert_key = false

  config.vm.define "vagrant1" do |vagrant1|
    vagrant1.vm.box = "ubuntu/trusty64"
    vagrant1.vm.network "forwarded_port", guest: 80, host: 8080
    vagrant1.vm.network "forwarded_port", guest: 443, host: 8443
  end
  config.vm.define "vagrant2" do |vagrant2|
    vagrant2.vm.box = "ubuntu/trusty64"
    vagrant2.vm.network "forwarded_port", guest: 80, host: 8081
    vagrant2.vm.network "forwarded_port", guest: 443, host: 8444
  end
  config.vm.define "vagrant3" do |vagrant3|
    vagrant3.vm.box = "ubuntu/trusty64"
    vagrant3.vm.network "forwarded_port", guest: 80, host: 8082
    vagrant3.vm.network "forwarded_port", guest: 443, host: 8445
  end
end
----
====


Vagrant 1.7+ defaults to using a different SSH key for each host.((("SSH", "using same key for each host")))
<<VAGRANTFILE_WITH_THREE_SERVERS>> contains the line to revert to the earlier behavior of using the same SSH key for each host:

----
config.ssh.insert_key = false
----


Using the same key on each host simplifies our Ansible setup because we can specify a single SSH key in the _ansible.cfg_ file. ((("ansible.cfg file", "host_key_checking value")))((("host_key_checking"))) You'll need to edit the `host_key_checking` value in your _ansible.cfg_. Your file should look like <<ANSIBLE_CFG_WITH_VAGRANT_KEY>>.

[[ANSIBLE_CFG_WITH_VAGRANT_KEY]]
.ansible.cfg
====
[source,ini]
----
[defaults]
inventory = inventory
remote_user = vagrant
private_key_file = ~/.vagrant.d/insecure_private_key
host_key_checking = False
----
====

For now, we'll assume that each of these servers can potentially be a web server, so
<<VAGRANTFILE_WITH_THREE_SERVERS>> maps ports 80 and 443 inside each Vagrant
machine to a port on the local machine.

You should be able to bring up the virtual machines by running the following:

----
$ vagrant up
----

If all went well, the output should look something like this:

----
Bringing machine 'vagrant1' up with 'virtualbox' provider...
Bringing machine 'vagrant2' up with 'virtualbox' provider...
Bringing machine 'vagrant3' up with 'virtualbox' provider...
...
    vagrant3: 80 => 8082 (adapter 1)
    vagrant3: 443 => 8445 (adapter 1)
    vagrant3: 22 => 2201 (adapter 1)
==> vagrant3: Booting VM...
==> vagrant3: Waiting for machine to boot. This may take a few minutes...
    vagrant3: SSH address: 127.0.0.1:2201
    vagrant3: SSH username: vagrant
    vagrant3: SSH auth method: private key
    vagrant3: Warning: Connection timeout. Retrying...
==> vagrant3: Machine booted and ready!
==> vagrant3: Checking for guest additions in VM...
==> vagrant3: Mounting shared folders...
    vagrant3: /vagrant => /Users/lorin/dev/oreilly-ansible/playbooks
----

Let's create an inventory file that contains these three machines.

First, we need to know what ports on the local machine map to the SSH port (22) inside each VM.((("SSH", "mapping SSH port in Vagrant machines to local machine")))((("vagrant ssh-config command"))) Recall that we can get that information by running the following:

----
$ vagrant ssh-config
----

The output should look something like this:

----
Host vagrant1
  HostName 127.0.0.1
  User vagrant
  Port 2222
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentityFile /Users/lorin/.vagrant.d/insecure_private_key
  IdentitiesOnly yes
  LogLevel FATAL

Host vagrant2
  HostName 127.0.0.1
  User vagrant
  Port 2200
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentityFile /Users/lorin/.vagrant.d/insecure_private_key
  IdentitiesOnly yes
  LogLevel FATAL

Host vagrant3
  HostName 127.0.0.1
  User vagrant
  Port 2201
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentityFile /Users/lorin/.vagrant.d/insecure_private_key
  IdentitiesOnly yes
  LogLevel FATAL
----

We can see that `vagrant1` uses port 2222, `vagrant2` uses port 2200, and `vagrant3` uses port 2201.


Modify your _hosts_ file so it looks like this:

----
vagrant1 ansible_host=127.0.0.1 ansible_port=2222
vagrant2 ansible_host=127.0.0.1 ansible_port=2200
vagrant3 ansible_host=127.0.0.1 ansible_port=2201
----


Now, make sure that you can access these machines. For example, to get
information about ((("network interface, getting information about on Vagrant machines")))the network interface for `vagrant2`, run the following:

----
$ ansible vagrant2 -a "ip addr show dev eth0"
----

On my machine, the output looks like this:

----
vagrant2 | success | rc=0 >>
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP
group default qlen 1000
    link/ether 08:00:27:fe:1e:4d brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::a00:27ff:fefe:1e4d/64 scope link
       valid_lft forever preferred_lft forever
----


=== Behavioral Inventory Parameters

To describe our Vagrant machines in the Ansible inventory file, we had to
explicitly ((("inventory", "multiple Vagrant machines", startref="ix_invenVag")))((("Vagrant", "configuring to bring up three hosts", startref="ix_Vagmulti")))specify the hostname (127.0.0.1) and port (2222, 2200, or 2201) that
Ansible's SSH client should connect to.((("inventory", "behavioral inventory parameters", id="ix_invenbip")))((("behavioral inventory parameters", id="ix_bip"))) Ansible calls these variables _behavioral inventory parameters_, and there are
several of them you can use when you need to override the Ansible defaults for a
host (see <<table3-1>>).

[[table3-1]]
.Behavioral inventory parameters
[options="header"]
|===============================================================================================================
|Name                        |Default          |Description
|ansible_host            |Name of host     |Hostname or IP address to SSH to
|ansible_port            |          22     |Port to SSH to
|ansible_user            |        Root     |User to SSH as
|ansible_password            | (_None_)         |Password to use for SSH authentication
|ansible_connection          |         smart   |How Ansible will connect to host (see the following section)
|ansible_private_key_file|(_None_)           |SSH private key to use for SSH authentication
|ansible_shell_type          |sh               |Shell to use for commands (see the following section)
|ansible_python_interpreter  |_/usr/bin/python_|Python interpreter on host (see the following section)
|ansible_*_interpreter       |(_None_)           |Like ansible_python_interpreter for other languages (see the following section)
|===============================================================================================================

For some of these options, the meaning is obvious from the name, but others require additional explanation.

==== ansible_connection

Ansible supports multiple _transports_, which are mechanisms that Ansible uses
to connect to the host.((("transports")))((("ansible_connection")))((("behavioral inventory parameters", "ansible_connection"))) The default transport, `smart`, will check whether the locally installed SSH client supports a feature called _ControlPersist_.((("smart transport")))((("SSH", "local client support for ControlPersist")))((("ControlPersist"))) If the SSH client supports ControlPersist, Ansible will use the local SSH client. If the SSH client doesn't support pass:[<span class="keep-together">ControlPersist</span>], the smart transport will fall back to using a Python-based SSH client library called _Paramiko_.((("Paramiko")))


==== ansible_shell_type

Ansible works by making SSH connections to remote machines and then invoking scripts.((("ansible_shell_type")))((("behavioral inventory parameters", "ansible_shell_type")))((("shells", "ansible_shell_type parameter"))) By default, Ansible assumes that the remote shell is the Bourne shell located at _/bin/sh_, and will generate the appropriate command-line parameters that work with Bourne shell.

Ansible also accepts +csh+, +fish+, and (on Windows) +powershell+ as valid values
for this parameter. I've never encountered a need for changing the shell type.

==== ansible_python_interpreter

Because the modules that ship with Ansible are implemented in Python 2, Ansible((("behavioral inventory parameters", "ansible_python_interpreter")))((("ansible_python_interpreter")))
needs to know the location of the Python interpreter on the remote machine.((("interpreters")))((("Python", "interpreter, location on remote machine"))) You
might need to change this if your remote host does not have a Python 2 interpreter
at _/usr/bin/python_. For example, if you are managing hosts that((("Arch Linux, hosts that run on"))) run Arch
Linux, you will need to change this to _/usr/bin/python2_, because Arch Linux
installs Python 3 at _/usr/bin/python_, and Ansible modules are not (yet) compatible
with Python 3.


////

TODO: Make sure we cover this or remove it

////
==== ansible_*_interpreter

If you are using a custom module that is not written in Python, you can use this parameter to specify the location of the interpreter (e.g., _/usr/bin/ruby_).((("ansible_*_interpreter")))
We'll cover this in <<custom_modules>>.

==== Changing Behavioral Parameter Defaults

You can override some of the behavioral parameter default values in the +defaults+
section of the _ansible.cfg_ file (<<defaults-ansiblecfg>>). Recall that we used this previously to change
the default SSH user.((("ansible.cfg file", "default behavioral inventory parameters, overriding")))((("behavioral inventory parameters", "changing defaults")))

[[defaults-ansiblecfg]]
.Defaults that can be overridden in ansible.cfg
[options="header"]
|======================================================
|Behavioral inventory parameter|ansible.cfg option
|ansible_port              |remote_port
|ansible_user              |remote_user
|ansible_private_key_file  |private_key_file
|ansible_shell_type            |executable (see the following paragraph)
|======================================================

////
See: ansible/lib/ansible/runner/__init__.py:853-859
////
The _ansible.cfg_ +executable+ config option is not exactly the same as the
+ansible_shell_type+ behavioral inventory parameter.((("executable (config option)"))) Instead, the executable
specifies the full path of the shell to use on the remote machine (e.g.,
_/usr/local/bin/fish_). Ansible will look at the name of the base name of this
path (in the case of _/usr/local/bin/fish_, the base name is _fish_) and use that
as the default value for +ansible_shell_type+.((("inventory", "behavioral inventory parameters", startref="ix_invenbip")))((("behavioral inventory parameters", startref="ix_bip")))


=== Groups and Groups and Groups

When performing configuration tasks, we typically want to perform actions on
groups of hosts, rather than on an individual host.((("groups", id="ix_group")))((("inventory", "groups", id="ix_invengr"))) Ansible automatically defines a group called `all` (or _*_), which includes all
of the hosts in the inventory. ((("all group")))For example, we can check whether the clocks on the
machines are roughly synchronized by running the following:
[source,console]
----
$ ansible all -a "date"
----

or
[source,console]
----
$ ansible '*' -a "date"
----

The output on my system looks like this:

----
vagrant3 | success | rc=0 >>
Sun Sep  7 02:56:46 UTC 2014

vagrant2 | success | rc=0 >>
Sun Sep  7 03:03:46 UTC 2014

vagrant1 | success | rc=0 >>
Sun Sep  7 02:56:47 UTC 2014
----

We can define our own groups in the inventory file.
Ansible uses the _.ini_ file format for inventory files. In the _.ini_ format,
configuration values are grouped together into sections.(((".ini file format", primary-sortas="ini file format")))

Here's how to specify that our vagrant hosts are in a group called
`vagrant`, along with the other example hosts we mentioned at the beginning of the chapter:

----
ontario.example.com
newhampshire.example.com
maryland.example.com
virginia.example.com
newyork.example.com
quebec.example.com
rhodeisland.example.com

[vagrant]
vagrant1 ansible_host=127.0.0.1 ansible_port=2222
vagrant2 ansible_host=127.0.0.1 ansible_port=2200
vagrant3 ansible_host=127.0.0.1 ansible_port=2201
----

We could have also listed the Vagrant hosts at the top, and then also in a group, like this:


----
maryland.example.com
newhampshire.example.com
newyork.example.com
ontario.example.com
quebec.example.com
rhodeisland.example.com
vagrant1 ansible_host=127.0.0.1 ansible_port=2222
vagrant2 ansible_host=127.0.0.1 ansible_port=2200
vagrant3 ansible_host=127.0.0.1 ansible_port=2201
virginia.example.com

[vagrant]
vagrant1
vagrant2
vagrant3

----

==== Example: Deploying a Django App

Imagine you're responsible for deploying a Django-based web application that
processes long-running jobs.((("inventory", "groups", "deploying a Django app (example)", id="ix_invengrDJ")))((("groups", "example, deploying a Django app", id="ix_groupex")))((("Django", "example, deploying a Django app", id="ix_Djangoapp"))) The app needs to support the following services:

* The actual Django web app itself, run by a Gunicorn HTTP server
* An Nginx web server, which will sit in front of Gunicorn and serve static
  assets
* A Celery task queue that will execute long-running jobs on behalf of the web
  app
* A RabbitMQ message queue that serves as the backend for Celery
* A Postgres database that serves as the persistent store

[NOTE]
====
In later chapters, we will work through a detailed example of deploying this
kind of Django-based application, although our example won't use Celery or RabbitMQ.
====

We need to deploy this application into different types of environments: production (the real thing), staging (for testing on hosts that our team has shared access to), and Vagrant (for local testing).

When we deploy to production,((("production, deploying Django app to"))) we want the entire system to respond quickly and be reliable, so we do the following:

 * Run the web application on multiple hosts for better performance and put a load balancer in front of them.
 * Run task queue servers on multiple hosts for better performance.
 * Put Gunicorn, Celery, RabbitMQ, and Postgres all on separate servers.
 * Use two Postgres hosts, a primary and a replica.

Assuming we have one load balancer, three web servers, three task queues, one RabbitMQ server, and two database servers, that's 10 hosts we need to deal with.((("load balancers")))

For our staging environment, imagine that we want to use fewer hosts than we do
in production in order ((("staging environment, deploying Django app to")))to save costs, especially since the staging environment
is going to see a lot less activity than production. Let's say we decide to use only two hosts for staging; we'll put the web server and task queue on one
staging host, and RabbitMQ and Postgres on the other.

For our local Vagrant environment, we decide to use three servers: one for the web app, one for a task queue, and one that will contain RabbitMQ and Postgres.((("Vagrant", "deploying Django app to")))

<<django_inventory_file>> shows a possible inventory file that groups our servers by environment (production, staging, Vagrant) and by function (web server, task queue, etc.).

[[django_inventory_file]]
.Inventory file for deploying a Django app
====
----
[production]
delaware.example.com
georgia.example.com
maryland.example.com
newhampshire.example.com
newjersey.example.com
newyork.example.com
northcarolina.example.com
pennsylvania.example.com
rhodeisland.example.com
virginia.example.com

[staging]
ontario.example.com
quebec.example.com

[vagrant]
vagrant1 ansible_host=127.0.0.1 ansible_port=2222
vagrant2 ansible_host=127.0.0.1 ansible_port=2200
vagrant3 ansible_host=127.0.0.1 ansible_port=2201

[lb]
delaware.example.com

[web]
georgia.example.com
newhampshire.example.com
newjersey.example.com
ontario.example.com
vagrant1

[task]
newyork.example.com
northcarolina.example.com
maryland.example.com
ontario.example.com
vagrant2

[rabbitmq]
pennsylvania.example.com
quebec.example.com
vagrant3

[db]
rhodeisland.example.com
virginia.example.com
quebec.example.com
vagrant3
----
====

We could have first listed all of the servers at the top of the inventory file,
without specifying a group, but that isn't necessary, and that would've made
this file even longer.

Note that we needed to specify the behavioral inventory parameters for the Vagrant instances only once.((("groups", "example, deploying a Django app", startref="ix_groupex")))((("Django", "example, deploying a Django app", startref="ix_Djangoapp")))((("inventory", "groups", "deploying a Django app (example)", startref="ix_invengrDJ")))

==== Aliases and Ports

We described our Vagrant ((("groups", "aliases and ports")))hosts like this:

----
[vagrant]
vagrant1 ansible_host=127.0.0.1 ansible_port=2222
vagrant2 ansible_host=127.0.0.1 ansible_port=2200
vagrant3 ansible_host=127.0.0.1 ansible_port=2201
----

The names +vagrant1+, +vagrant2+, and +vagrant3+ here are _aliases_.((("aliases (for hostnames)")))((("hostnames"))) They are not the real
hostnames, but instead are useful names for referring to these hosts.

Ansible supports using +<hostname>:<port>+ syntax when specifying hosts,((("ports", "in hostnames"))) so we
could replace the line that contains +vagrant1+ with +127.0.0.1:2222+. However, we can't actually run what you see in <<vagrant-doesnt-work>>.

[[vagrant-doesnt-work]]
.This doesn't work
====
----
[vagrant]
127.0.0.1:2222
127.0.0.1:2200
127.0.0.1:2201
----
====

The reason is that Ansible's inventory can associate only a single host with _127.0.0.1_, so the Vagrant group would contain only one host instead of three.

==== Groups of Groups

Ansible also allows you to define groups that are made up of other groups. For
example, both the web servers and the task queue servers will need to have
Django and its dependencies.((("groups", "of groups")))((("inventory", "groups", "groups of groups"))) We might find it useful to define a `django` group that
contains both of these two groups. You would add this to the
inventory file:

----
[django:children]
web
task
----


Note that the syntax changes when you are specifying a group of groups, as
opposed to a group of hosts. That's so Ansible knows to interpret `web` and
`task` as groups and not as hosts.

==== Numbered Hosts (Pets versus Cattle)

The inventory file shown in <<django_inventory_file>> looks complex.((("numbered hosts")))((("inventory", "groups", "numbered hosts (pets vs. cattle)"))) In reality, it describes
only 15 hosts, which doesn't sound like a large number in this cloudy
scale-out world. However, even dealing with 15 hosts in the inventory file
can be cumbersome, because each host has a completely different hostname.((("servers", "treating as pets versus cattle")))((("pets versus cattle")))((("cattle versus pets")))


Bill Baker of Microsoft came up with the distinction between treating servers as
_pets_ versus treating them like _cattle_.footnote:[This term has been
popularized by http://bit.ly/1P3nHB2[Randy Bias of Cloudscaling].] We
give pets distinctive names, and we treat and care for them as individuals. On
the other hand, when we discuss cattle, we refer to them by identification number.

The cattle approach is much more scalable, and Ansible supports it well by
supporting numeric patterns. For example, if your 20 servers are named _web1.example.com_, _web2.example.com_, and so on, then you can specify them in the inventory file like this:

----
[web]
web[1:20].example.com
----


If you prefer to have a leading zero (e.g., _web01.example.com_), then specify a
leading zero in the range, like this:

----
[web]
web[01:20].example.com
----

Ansible also supports using alphabetic characters to specify ranges.((("alphabetic characters, specifying ranges with"))) If you
want to use the convention _web-a.example.com_, _web-b.example.com_, and so on, for your 20 servers, then you can do this:

----
[web]
web-[a-t].example.com
----



////
TODO: I am here, discuss numbered hosts, pets vs. cattle
http://www.slideshare.net/randybias/architectures-for-open-and-scalable-clouds
////

=== Hosts and Group Variables: Inside the Inventory

Recall how we specified behavioral inventory ((("groups", startref="ix_group")))((("inventory", "groups", startref="ix_invengr")))parameters for Vagrant hosts:

----
vagrant1 ansible_host=127.0.0.1 ansible_port=2222
vagrant2 ansible_host=127.0.0.1 ansible_port=2200
vagrant3 ansible_host=127.0.0.1 ansible_port=2201
----

Those parameters are variables that have special meaning to Ansible.((("variables", "host and group variables in inventory")))((("inventory", "host and group variables in"))) We can also
define arbitrary variable names and associated values on hosts. For example,
we could define a variable named `color`  and set it to a value for each server:

----
newhampshire.example.com color=red
maryland.example.com color=green
ontario.example.com color=blue
quebec.example.com color=purple
----

This variable can then be used in a playbook, just like any other variable.

Personally, I don't often attach variables to specific hosts. On the other hand,
I often associate variables with groups.

Circling back to our Django example, the web application and task queue service
need to communicate with RabbitMQ and Postgres. We'll assume that access to the
Postgres database is secured both at the network layer (so only the web
application and the task queue can reach the database) as well as by
username and password, whereas RabbitMQ is secured only by the network layer.

To set everything up, we need to do the following:

* Configure the web servers with the hostname, port, username, password of the primary Postgres server, and name of the database.
* Configure the task queues with the hostname, port, username, password of the primary Postgres server, and the name of the database.
* Configure the web servers with the hostname and port of the RabbitMQ server.
* Configure the task queues with the hostname and port of the RabbitMQ server.
* Configure the primary Postgres server with the hostname, port, and username and password of the replica Postgres server (production only).

This configuration info varies by environment, so it makes sense to define these as group variables on the production,((("groups", "specifying group variables in inventory"))) staging, and Vagrant groups. <<spec-group-vars>> shows one way to specify this information as group variables in the inventory file.

////
TODO: Verify quoted strings work in ini file
////
[[spec-group-vars]]
.Specifying group variables in inventory
====
----
[all:vars]
ntp_server=ntp.ubuntu.com

[production:vars]
db_primary_host=rhodeisland.example.com
db_primary_port=5432
db_replica_host=virginia.example.com
db_name=widget_production
db_user=widgetuser
db_password=pFmMxcyD;Fc6)6
rabbitmq_host=pennsylvania.example.com
rabbitmq_port=5672


[staging:vars]
db_primary_host=quebec.example.com
db_primary_port=5432
db_name=widget_staging
db_user=widgetuser
db_password=L@4Ryz8cRUXedj
rabbitmq_host=quebec.example.com
rabbitmq_port=5672

[vagrant:vars]
db_primary_host=vagrant3
db_primary_port=5432
db_name=widget_vagrant
db_user=widgetuser
db_password=password
rabbitmq_host=vagrant3
rabbitmq_port=5672

----
====

////
TODO: Put a note about security concerns with plaintext passwords in this repo.
////

Note how group variables are organized into sections named +[<group name>:vars]+. Also note how we took advantage of the +all+ group that Ansible creates automatically to specify variables that don't change across hosts.((("all group")))

=== Host and Group Variables: In Their Own Files

The inventory file is a reasonable place to put host and group variables if you
don't have too many hosts.((("inventory", "host and group variables in their own files")))((("variables", "host and group variables in their own files"))) But as your inventory gets larger, it gets more
difficult to manage variables this way.((("groups", "group variables in their own files")))


////
TODO: Check if booleans are parsed as booleans or strings
////

Additionally, though Ansible variables can hold Booleans, strings, lists, and dictionaries, in an inventory file, you can specify only Booleans and strings.

Ansible offers a more scalable approach to keep track of host and group
variables: you can create a separate variable file for each host and each group.
Ansible expects these variable files to be in YAML format.

Ansible looks for host variable files in a directory called _host_vars_
and group variable files in a directory called _group_vars_. ((("host variable files")))Ansible expects
these directories to be either in the directory that contains your playbooks or
in the directory adjacent to your inventory file. In our case, those two
directories are the same.

For example, if I had a directory containing my playbooks at
_/home/lorin/playbooks/_ with an inventory file at
_/home/lorin/playbooks/hosts_, then I would put variables for
the _quebec.example.com_ host in the file _/home/lorin/playbooks/host_vars/quebec.example.com_, and
I would put variables for the production group in the file
_/home/lorin/playbooks/group_vars/production_.

<<group-vars-prod>> shows what the _/home/lorin/playbooks/group_vars/production_ file would
look like.

[[group-vars-prod]]
.group_vars/production
====
[source,yaml+jinja]
----
db_primary_host: rhodeisland.example.com
db_primary_port=5432
db_replica_host: virginia.example.com
db_name: widget_production
db_user: widgetuser
db_password: pFmMxcyD;Fc6)6
rabbitmq_host:pennsylvania.example.com
rabbitmq_port=5672
----
====

Note that we could also use YAML dictionaries to represent these
values, as((("YAML", "using dictionaries to represent group variables"))) shown in <<prod-with-dicts>>.

[[prod-with-dicts]]
.group_vars/production, with dictionaries
====
[source,yaml+jinja]
----
db:
    user: widgetuser
    password: pFmMxcyD;Fc6)6
    name: widget_production
    primary:
        host: rhodeisland.example.com
        port: 5432
    replica:
        host: virginia.example.com
        port: 5432

rabbitmq:
    host: pennsylvania.example.com
    port: 5672
----
====

If we choose YAML dictionaries, that changes the way we access the variables:

----
{{ db_primary_host }}
----

versus:

----
{{ db.primary.host }}
----


If you want to break things out even further, Ansible will allow you to define _group_vars/production_ as a directory instead of a file, and let you place multiple YAML files that contain variable definitions. For example, we could put the database-related variables in one file and the RabbitMQ-related variables in another file, as shown in Examples pass:[<a href="#example3-9">3-9</a>] and pass:[<a href="#example3-10">3-10</a>].

[[example3-9]]
.group_vars/production/db
====
[source,yaml+jinja]
----
db:
    user: widgetuser
    password: pFmMxcyD;Fc6)6
    name: widget_production
    primary:
        host: rhodeisland.example.com
        port: 5432
    replica:
        host: virginia.example.com
        port: 5432
----
====

[[example3-10]]
.group_vars/production/rabbitmq
====
[source,yaml+jinja]
----
rabbitmq:
    host: pennsylvania.example.com
    port: 6379
----
====

In general, I find it's better to keep things simple rather than split
variables out across too many files.

////

See:

http://docs.ansible.com/intro_inventory.html#splitting-out-host-and-group-specific-data

////

=== Dynamic Inventory
////
http://docs.ansible.com/intro_dynamic_inventory.html
////

Up until this point, we've been explicitly specifying all of our hosts in our hosts inventory file.((("dynamic inventory", id="ix_dyninv")))((("inventory", "dynamic", id="ix_invendyn"))) However, you might have a system external to Ansible that keeps track of your hosts. For example, if your hosts run on Amazon EC2, then EC2 tracks information about your hosts for you, and you can retrieve this information through EC2's web interface, its Query API, or through command-line tools such as `awscli`. Other cloud providers have similar interfaces. Or, if you're managing your own servers and are using an automated provisioning system such as Cobbler or Ubuntu Metal as a Service (MAAS), then your provisioning system is already keeping track of your servers. Or, maybe you have one of those fancy configuration management databases (CMDBs) where all of this information lives.

You don't want to manually duplicate this information in your hosts file, because eventually that file will not jibe with your external
system, which is the true source of information about your hosts.  Ansible supports a feature called _dynamic inventory_ that allows you to avoid this duplication.

If the inventory file is marked executable, ((("executable, marking inventory file as")))Ansible will assume it is a dynamic inventory script and will execute the file instead of reading it.

[TIP]
====
To mark a file as((("chmod &#x2b;x command"))) executable, use the +chmod +x+ command. For example:
[source,console]
----
$ chmod +x dynamic.py
----
====

==== The Interface for a Dynamic Inventory Script

An Ansible dynamic inventory script must support((("dynamic inventory", "interface for dynamic inventory scripts")))((("--host=&lt;hostname&gt; command-line flag", primary-sortas="host")))((("--list command-line flag", primary-sortas="list"))) two command-line flags:

* +--host=<hostname>+ for showing host details
* +--list+ for listing groups

===== Showing host details

To get the details of the individual host, Ansible will call ((("dynamic inventory", "interface for dynamic inventory scripts", "showing host details")))the inventory
script like this:
[source,console]
----
$ ./dynamic.py --host=vagrant2
----

The output should contain any host-specific variables, including behavioral
parameters, like this:
[source,json]
----
{ "ansible_host": "127.0.0.1", "ansible_port": 2200,
  "ansible_user": "vagrant"}
----

The output is a single JSON object; the names are variable names, and the
values are the variable values.

===== Listing groups

Dynamic inventory scripts need to be able to list all of the groups, and details
about the individual hosts.((("dynamic inventory", "interface for dynamic inventory scripts", "listing groups")))((("groups", "listing in dynamic inventory scripts"))) For example, if our script is called _dynamic.py_,
Ansible will call it like this to get a list of all of the groups:
[source,console]
----
$ ./dynamic.py --list
----

The output should look something like this:
[source,json]
----
{"production": ["delaware.example.com", "georgia.example.com",
                "maryland.example.com", "newhampshire.example.com",
                "newjersey.example.com", "newyork.example.com",
                "northcarolina.example.com", "pennsylvania.example.com",
                "rhodeisland.example.com", "virginia.example.com"],
 "staging": ["ontario.example.com", "quebec.example.com"],
 "vagrant": ["vagrant1", "vagrant2", "vagrant3"],
 "lb": ["delaware.example.com"],
 "web": ["georgia.example.com", "newhampshire.example.com",
         "newjersey.example.com", "ontario.example.com", "vagrant1"]
 "task": ["newyork.example.com", "northcarolina.example.com",
          "ontario.example.com", "vagrant2"],
 "rabbitmq": ["pennsylvania.example.com", "quebec.example.com", "vagrant3"],
 "db": ["rhodeisland.example.com", "virginia.example.com", "vagrant3"]
}
----

The output is a single JSON object; the names are Ansible group names, and
the values are arrays of hostnames.

As an optimization, the +--list+ command can contain the values of the host
variables for all of the hosts, which saves Ansible the trouble of making a
separate +--host+ invocation to retrieve the variables for the individual hosts.

To take advantage of this optimization, the +--list+ command should return a key
named +_meta+ that contains the variables for each host,((("_meta key"))) in this form:
[source,json]
----
"_meta" :
  { "hostvars" :
    "vagrant1" : { "ansible_host": "127.0.0.1", "ansible_port": 2222,
                   "ansible_user": "vagrant"},
    "vagrant2": { "ansible_host": "127.0.0.1", "ansible_port": 2200,
                   "ansible_user": "vagrant"},
    ...
}
----

==== Writing a Dynamic Inventory Script

One of the handy features of Vagrant is that you can see which machines are currently running by using the +vagrant status+ command.((("dynamic inventory", "writing a script")))((("vagrant status command"))) Assuming we have a Vagrant file that looks like <<VAGRANTFILE_WITH_THREE_SERVERS>>, if we run +vagrant status+, the output would look like <<VAGRANT_STATUS>>.

.Output of vagrant status
[[VAGRANT_STATUS]]
====
[source,console]
----
$ vagrant status
Current machine states:

vagrant1                  running (virtualbox)
vagrant2                  running (virtualbox)
vagrant3                  running (virtualbox)

This environment represents multiple VMs. The VMs are all listed
above with their current state. For more information about a specific
VM, run `vagrant status NAME`.
----
====


Because Vagrant already keeps track of machines for us, there's no need for us to
write a list of the Vagrant machines in an Ansible inventory file. Instead, we can write a
dynamic inventory script that queries Vagrant about which machines are
running. Once we've set up a dynamic inventory script for Vagrant, even if we alter our
Vagrantfile to run different numbers of Vagrant machines, we won't need to edit
an Ansible inventory file.


Let's work through an example of creating a dynamic inventory script
that retrieves the details about hosts from Vagrant.footnote:[Yes, there's a Vagrant dynamic inventory script included with Ansible already, but it's helpful to go through the exercise.] Our dynamic inventory script is going to need to invoke the +vagrant status+
command. The output shown in <<VAGRANT_STATUS>> is designed for humans to read,
rather than for machines to parse.((("vagrant status --machine-readable command"))) We can get a list of running
hosts in a format that is easier to parse with the +--machine-readable+ flag, like so:
[source,console]
----
$ vagrant status --machine-readable
----

The output looks like this:
[source,console]
----
1474694768,vagrant1,metadata,provider,virtualbox
1474694768,vagrant2,metadata,provider,virtualbox
1474694768,vagrant3,metadata,provider,virtualbox
1410577818,vagrant1,state,running
1410577818,vagrant1,state-human-short,running
1410577818,vagrant1,state-human-long,The VM is running. To stop this VM%!(VAGRANT
_COMMA) you can run `vagrant halt` to\nshut it down forcefully%!(VAGRANT_COMMA)
or you can run `vagrant suspend` to simply\nsuspend the virtual machine. In
either case%!(VAGRANT_COMMA) to restart it again%!(VAGRANT_COMMA)\nsimply run
`vagrant up`.
1410577818,vagrant2,state,running
1410577818,vagrant2,state-human-short,running
1410577818,vagrant2,state-human-long,The VM is running. To stop this VM%!(VAGRANT
_COMMA) you can run `vagrant halt` to\nshut it down forcefully%!(VAGRANT_COMMA)
or you can run `vagrant suspend` to simply\nsuspend the virtual machine. In 
either case%!(VAGRANT_COMMA) to restart it again%!(VAGRANT_COMMA)\nsimply run
`vagrant up`.
1410577818,vagrant3,state,running
1410577818,vagrant3,state-human-short,running
1410577818,vagrant3,state-human-long,The VM is running. To stop this VM%!(VAGRANT
_COMMA) you can run `vagrant halt` to\nshut it down forcefully%!(VAGRANT_COMMA)
or you can run `vagrant suspend` to simply\nsuspend the virtual machine. In
either case%!(VAGRANT_COMMA) to restart it again%!(VAGRANT_COMMA)\nsimply
run `vagrant up`.
----

To get details about ((("vagrant ssh-config command")))a particular Vagrant machine, say, +vagrant2+, we would run this:
[source,console]
----
$ vagrant ssh-config vagrant2
----

The output looks like this:
[source,console]
----
Host vagrant2
  HostName 127.0.0.1
  User vagrant
  Port 2200
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentityFile /Users/lorin/.vagrant.d/insecure_private_key
  IdentitiesOnly yes
  LogLevel FATAL
----


Our dynamic inventory script will need to call these commands, parse the outputs,
and output the appropriate JSON. We can use the Paramiko library to parse the
output of +vagrant ssh-config+. ((("Paramiko", "using to parse output of vagrant ssh-config")))((("vagrant ssh-config command", "parsing output with Paramiko library")))Here's an interactive Python session that shows
how to use the Paramiko library to do this:
[source,python]
----
>>> import subprocess
>>> import paramiko
>>> cmd = "vagrant ssh-config vagrant2"
>>> p = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)
>>> config = paramiko.SSHConfig()
>>> config.parse(p.stdout)
>>> config.lookup("vagrant2")
{'identityfile': ['/Users/lorin/.vagrant.d/insecure_private_key'],
 'loglevel': 'FATAL', 'hostname': '127.0.0.1', 'passwordauthentication': 'no',
 'identitiesonly': 'yes', 'userknownhostsfile': '/dev/null', 'user': 'vagrant',
 'stricthostkeychecking': 'no', 'port': '2200'}
----

[NOTE]
====
You need to install the Python Paramiko library in order to use this script. You can do ((("Paramiko", "installing with pip")))this with pip:
[source,console]
----
$ sudo pip install paramiko
----
====


////
TODO: Credit original vagrant.py script
////

<<vagrant-py>> shows our complete _vagrant.py_ script. 
[[vagrant-py]]
.vagrant.py
====
[source,python]
----
include::playbooks/inventory/vagrant.py[]
----
====

==== Preexisting Inventory Scripts

Ansible ships with several dynamic inventory scripts that you can use.((("dynamic inventory", "preexisting inventory scripts")))
I can never figure out where my package manager installs these files,
so I just grab the ones I need directly off GitHub.((("GitHub repositories", "for Ansible dynamic inventory scripts"))) You can grab these by
going to the https://github.com/ansible/ansible[Ansible GitHub repo] and
browsing to the _contrib/inventory_ directory.

Many of these inventory scripts have an accompanying configuration file. In
<<cloud>>, we'll discuss the Amazon EC2 inventory script in more detail.((("dynamic inventory", startref="ix_dyninv")))((("inventory", "dynamic", startref="ix_invendyn")))

=== Breaking the Inventory into Multiple Files

If you want to have both a regular inventory file and a dynamic inventory script
(or, really, any combination of static and dynamic inventory files), just put them all in the same directory and configure Ansible to use that directory as the inventory.((("inventory", "breaking into multiple files")))((("ansible.cfg file", "inventory parameter")))((("ansible-doc command-line tool", "-i flag"))) You can do this either via the +inventory+ parameter in _ansible.cfg_ or by using the +-i+ flag on the command line. Ansible will process all of the files and merge the results into a single inventory.

For example, our directory structure could look like this: _inventory/hosts_ and _inventory/vagrant.py_.

Our _ansible.cfg_ file would contain these lines:
[source,ini]
----
[defaults]
inventory = inventory
----


=== Adding Entries at Runtime with add_host and group_by

Ansible will let you add hosts and groups to the inventory during the execution
of a playbook.((("inventory", "adding entries at runtime with add_host and group_by", id="ix_invenadd")))

==== add_host

The `add_host` module adds a host to the inventory.((("add_host module")))((("hosts", "adding to inventory at runtime with add_host")))((("inventory", "adding entries at runtime with add_host and group_by", "add_host module"))) This module is useful if you're
using Ansible to provision new virtual machine instances inside an
infrastructure-as-a-service cloud.

.Why Do I Need add_host if I'm Using Dynamic Inventory?
****
Even if you're using dynamic inventory scripts, the `add_host` module is useful
for scenarios where you start up new virtual machine instances and
configure those instances in the same playbook.

If a new host comes online while a playbook is executing, the dynamic inventory
script will not pick up this new host. This is because the dynamic inventory script
is executed at the beginning of the playbook, so if any new hosts are added
while the playbook is executing, Ansible won't see them.

We'll cover a cloud computing example that uses the `add_host` module in <<cloud>>.
****

Invoking the module looks like this:
[source,yaml+jinja]
----
add_host name=hostname groups=web,staging myvar=myval
----

Specifying the list of groups and additional variables is optional.

Here's the +add_host+ command in action, bringing up a new Vagrant machine and
then configuring the machine:
[source,yaml+jinja]
----
- name: Provision a vagrant machine
  hosts: localhost
  vars:
    box: trusty64
  tasks:
    - name: create a Vagrantfile
      command: vagrant init {{ box }} creates=Vagrantfile

    - name: Bring up a vagrant machine
      command: vagrant up

    - name: add the vagrant machine to the inventory
      add_host: >
            name=vagrant
            ansible_host=127.0.0.1
            ansible_port=2222
            ansible_user=vagrant
            ansible_private_key_file=/Users/lorin/.vagrant.d/
            insecure_private_key

- name: Do something to the vagrant machine
  hosts: vagrant
  become: yes
  tasks:
    # The list of tasks would go here
    - ...
----

[NOTE]
====
The `add_host` module adds the host only for the duration of the execution of the playbook. It does not modify your inventory file.
====

When I do provisioning inside my playbooks, I like to split it into two
plays. The first play runs against +localhost+ and provisions the hosts, and the
second play configures the hosts.

Note that we use the +creates=Vagrantfile+ parameter in this task:

----
- name: create a Vagrantfile
  command: vagrant init {{ box }} creates=Vagrantfile
----

This tells Ansible that if the _Vagrantfile_ file is present, the host
is already in the correct state, and there is no need to run the command again. It's a way of achieving idempotence in a playbook that invokes the command module, by ensuring that the (potentially nonidempotent) command is run only once.

==== group_by

Ansible also allows you to create new groups during execution of a playbook,
using the `group_by` module.((("inventory", "adding entries at runtime with add_host and group_by", "group_by module")))((("group_by module"))) This lets you create a group based on the
value of a variable that has been set on each host, which Ansible refers to as
a _fact_.footnote:[We cover facts in more detail in <<variables_and_facts>>.]

If Ansible fact gathering is enabled, Ansible will associate a set of
variables with a host. For example, the `ansible_machine` variable will be
`i386` for 32-bit x86 machines and `x86_64` for 64-bit x86 machines. If Ansible
is interacting with a mix of such hosts, we can create `i386` and `x86_64`
groups with the task.

Or, if we want to group our hosts by((("Linux distributions", "creating ad hoc groups based on"))) Linux distribution (e.g., Ubuntu, CentOS),
we can use the `ansible_distribution` fact:
[source,yaml+jinja]
----
- name: create groups based on Linux distribution
  group_by: key={{ ansible_distribution }}
----

In <<GROUP_BY_EXAMPLE>>, we use +group_by+ to create separate
groups for our Ubuntu hosts and our CentOS hosts, and then we use the `apt`
module to install packages onto Ubuntu and the `yum` module to install packages
into CentOS.

[[GROUP_BY_EXAMPLE]]
.Creating ad hoc groups based on Linux distribution
====
[source,yaml+jinja]
----
- name: group hosts by distribution
  hosts: myhosts
  gather_facts: True
  tasks:
    - name: create groups based on distro
      group_by: key={{ ansible_distribution }}

- name: do something to Ubuntu hosts
  hosts: Ubuntu
  tasks:
    - name: install htop
      apt: name=htop
    # ...

- name: do something else to CentOS hosts
  hosts: CentOS
  tasks:
    - name: install htop
      yum: name=htop
    # ...

----
====


Although using +group_by+ is one way to achieve conditional behavior in Ansible,
I've never found much use for it. In <<deploying_mezzanine>>, you'll see an
example of how to use the +when+ task parameter to take different
actions based on variables.

////
TODO: Reword awkwardness here
////
That about does it for Ansible's inventory. The next chapter covers how to use variables. See <<connections-ssh>> for more details about _ControlPersist_, also known as SSH multiplexing.((("inventory", "adding entries at runtime with add_host and group_by", startref="ix_invenadd")))((("inventory", startref="ix_inven")))
